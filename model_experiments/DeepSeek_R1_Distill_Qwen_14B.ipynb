{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import textwrap\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zly20\\.cache\\huggingface\\hub\n"
     ]
    }
   ],
   "source": [
    "from transformers import TRANSFORMERS_CACHE\n",
    "\n",
    "print(TRANSFORMERS_CACHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\"\n",
    "cache_dir = \"E:/transformers.cache\"  # Specify your desired cache directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d341313207674cbba1f24c5bbfd1cd6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    use_cache=True,\n",
    "    cache_dir=cache_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question):\n",
    "    # Tokenize input question\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Start timing\n",
    "    start_time = time.time() \n",
    "\n",
    "    # Generate response\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "        num_beams=1,\n",
    "    )\n",
    "\n",
    "    end_time = time.time()  # End timing\n",
    "\n",
    "    # Count total tokens generated\n",
    "    num_tokens = generated_ids.shape[1]  # Number of generated tokens\n",
    "\n",
    "    # Compute tokens per second\n",
    "    tps = num_tokens / (end_time - start_time)\n",
    "    print(f\"Tokens per second: {tps:.2f}\")\n",
    "    \n",
    "    # Decode output\n",
    "    outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    # empty cache so that you do generate more results w/o memory crashing\n",
    "    # particularly important on Colab â€“ memory management is much more straightforward\n",
    "    # when running on an inference service\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens per second: 43.94\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "\n",
    "Consider the multiple linear regression model:\n",
    "\n",
    "$$Y_i = \\beta^T x_i + \\epsilon_i$$\n",
    "\n",
    "for $i = 1, ..., n$, where $\\epsilon_i$'s are $iid$ $N(0, \\sigma^2)$, $\\beta = (\\beta_0, \\beta_1, ..., \\beta_p)^T$ is the vector of regression coefficients, and $x_i = (1, x_{i1}, x_{i2}, ..., x_{ip})^T$ is the vector of predictors for the i-th observation.\n",
    "\n",
    "\n",
    "## QUESTION 2 (a)\n",
    "\n",
    "Write down the likelihood function and the log-likelihood function for the multiple linear regression model.\n",
    "\n",
    "\n",
    "## QUESTION 2 (a) Solution:\n",
    "\n",
    "Write your solution here in interprtable LaTex style...\n",
    "\n",
    "\"\"\"\n",
    "answer = generate_answer(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"\\n\\nConsider the multiple linear regression model:\\n\\n$$Y_i = \\x08eta^T x_i + \\\\epsilon_i$$\\n\\nfor $i = 1, ..., n$, where $\\\\epsilon_i$'s are $iid$ $N(0, \\\\sigma^2)$, $\\x08eta = (\\x08eta_0, \\x08eta_1, ..., \\x08eta_p)^T$ is the vector of regression coefficients, and $x_i = (1, x_{i1}, x_{i2}, ..., x_{ip})^T$ is the vector of predictors for the i-th observation.\\n\\n\\n## QUESTION 2 (a)\\n\\nWrite down the likelihood function and the log-likelihood function for the multiple linear regression model.\\n\\n\\n## QUESTION 2 (a) Solution:\\n\\nWrite your solution here in interprtable LaTex style...\\n\\n</think>\\n\\nTo derive the likelihood function and the log-likelihood function for the multiple linear regression model, we start by considering the probability density function of the normal distribution. Given that the error terms $\\\\epsilon_i$ are independently and identically distributed (iid) as $N(0, \\\\sigma^2)$, the probability density function for each $\\\\epsilon_i$ is:\\n\\n$$\\nf(\\\\epsilon_i) = \\\\frac{1}{\\\\sqrt{2\\\\pi\\\\sigma^2}} \\\\exp\\\\left(-\\\\frac{\\\\epsilon_i^2}{2\\\\sigma^2}\\\\right)\\n$$\\n\\nSince the observations are independent, the joint likelihood function for all observations is the product of the individual likelihoods. Substituting $\\\\epsilon_i = Y_i - \\\\beta^T x_i$ into the probability density function, the likelihood function $L(\\\\beta, \\\\sigma^2)$ can be written as:\\n\\n$$\\nL(\\\\beta, \\\\sigma^2) = \\\\prod_{i=1}^{n} \\\\frac{1}{\\\\sqrt{2\\\\pi\\\\sigma^2}} \\\\exp\\\\left(-\\\\frac{(Y_i - \\\\beta^T x_i)^2}{2\\\\sigma^2}\\\\right)\\n$$\\n\\nTaking the natural logarithm of the likelihood function to obtain the log-likelihood function $\\\\ell(\\\\beta, \\\\sigma^2)$:\\n\\n$$\\n\\\\ell(\\\\beta, \\\\sigma^2) = \\\\sum_{i=1}^{n} \\\\left[ -\\\\frac{1}{2} \\\\ln(2\\\\pi\\\\sigma^2) - \\\\frac{(Y_i - \\\\beta^T x_i)^2}{2\\\\sigma^2} \\\\right]\\n$$\\n\\nSimplifying the expression:\\n\\n$$\\n\\\\ell(\\\\beta, \\\\sigma^2) = -\\\\frac{n}{2} \\\\ln(2\\\\pi\\\\sigma^2) - \\\\frac{1}{2\\\\sigma^2} \\\\sum_{i=1}^{n} (Y_i - \\\\beta^T x_i)^2\\n$$\\n\\nThus, the log-likelihood function is:\\n\\n$$\\n\\\\boxed{ \\\\ell(\\\\beta, \\\\sigma^2) = -\\\\frac{n}{2} \\\\ln(2\\\\pi\\\\sigma^2) - \\\\frac{1}{2\\\\sigma^2} \\\\sum_{i=1}^{n} (Y_i - \\\\beta^T x_i)^2 }\\n$$\"]\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"### Task\n",
    "Generate a SQL query to answer [QUESTION]{question}[/QUESTION]\n",
    "\n",
    "### Instructions\n",
    "- If you cannot answer the question with the available database schema, return 'I do not know'\n",
    "- Remember that revenue is price multiplied by quantity\n",
    "- Remember that cost is supply_price multiplied by quantity\n",
    "\n",
    "### Database Schema\n",
    "This query will run on a database whose schema is represented in this string:\n",
    "CREATE TABLE products (\n",
    "  product_id INTEGER PRIMARY KEY, -- Unique ID for each product\n",
    "  name VARCHAR(50), -- Name of the product\n",
    "  price DECIMAL(10,2), -- Price of each unit of the product\n",
    "  quantity INTEGER  -- Current quantity in stock\n",
    ");\n",
    "\n",
    "CREATE TABLE customers (\n",
    "   customer_id INTEGER PRIMARY KEY, -- Unique ID for each customer\n",
    "   name VARCHAR(50), -- Name of the customer\n",
    "   address VARCHAR(100) -- Mailing address of the customer\n",
    ");\n",
    "\n",
    "CREATE TABLE salespeople (\n",
    "  salesperson_id INTEGER PRIMARY KEY, -- Unique ID for each salesperson\n",
    "  name VARCHAR(50), -- Name of the salesperson\n",
    "  region VARCHAR(50) -- Geographic sales region\n",
    ");\n",
    "\n",
    "CREATE TABLE sales (\n",
    "  sale_id INTEGER PRIMARY KEY, -- Unique ID for each sale\n",
    "  product_id INTEGER, -- ID of product sold\n",
    "  customer_id INTEGER,  -- ID of customer who made purchase\n",
    "  salesperson_id INTEGER, -- ID of salesperson who made the sale\n",
    "  sale_date DATE, -- Date the sale occurred\n",
    "  quantity INTEGER -- Quantity of product sold\n",
    ");\n",
    "\n",
    "CREATE TABLE product_suppliers (\n",
    "  supplier_id INTEGER PRIMARY KEY, -- Unique ID for each supplier\n",
    "  product_id INTEGER, -- Product ID supplied\n",
    "  supply_price DECIMAL(10,2) -- Unit price charged by supplier\n",
    ");\n",
    "\n",
    "-- sales.product_id can be joined with products.product_id\n",
    "-- sales.customer_id can be joined with customers.customer_id\n",
    "-- sales.salesperson_id can be joined with salespeople.salesperson_id\n",
    "-- product_suppliers.product_id can be joined with products.product_id\n",
    "\n",
    "### Answer\n",
    "Given the database schema, here is the SQL query that answers [QUESTION]{question}[/QUESTION]\n",
    "[SQL]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query(question):\n",
    "    updated_prompt = prompt.format(question=question)\n",
    "    inputs = tokenizer(updated_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Start timing\n",
    "    start_time = time.time() \n",
    "\n",
    "    # Generate response\n",
    "    generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "        num_beams=1,\n",
    "    )\n",
    "\n",
    "    end_time = time.time()  # End timing\n",
    "\n",
    "    # Count total tokens generated\n",
    "    num_tokens = generated_ids.shape[1]  # Number of generated tokens\n",
    "\n",
    "    # Compute tokens per second\n",
    "    tps = num_tokens / (end_time - start_time)\n",
    "    print(f\"Tokens per second: {tps:.2f}\")\n",
    "    \n",
    "    outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    # empty cache so that you do generate more results w/o memory crashing\n",
    "    # particularly important on Colab â€“ memory management is much more straightforward\n",
    "    # when running on an inference service\n",
    "    return sqlparse.format(outputs[0].split(\"[SQL]\")[-1], reindent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens per second: 64.17\n"
     ]
    }
   ],
   "source": [
    "question = \"What was the total number of goods sold in Canada last quarter?\"\n",
    "generated_sql = generate_query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SELECT SUM(s.quantity) AS total_goods_sold\n",
      "FROM sales s\n",
      "JOIN products p ON s.product_id = p.product_id\n",
      "WHERE s.sale_date >= DATEADD(qq, -1, GETDATE())\n",
      "  AND s.sale_date < DATEADD(qq, 0, GETDATE())\n",
      "  AND p.name LIKE '%Canada%';\n",
      "\n",
      "[/SQL] </think> To determine the total number of goods sold in Canada last quarter,\n",
      "                                                                           we need to query the sales table\n",
      "and filter the data based on the product name\n",
      "and the sale date. Here's the step-by-step explanation of the SQL query:\n",
      "\n",
      "1. **SELECT SUM(s.quantity)**: This part of the query calculates the total number of goods sold by summing up the quantity from the sales table.\n",
      "\n",
      "2. **AS total_goods_sold**: This aliases the result column for clarity, labeling it as \"total_goods_sold\".\n",
      "\n",
      "3. **FROM sales s**: This specifies the table we are querying, which is the \"sales\" table aliased as \"s\".\n",
      "\n",
      "4. **JOIN products p ON s.product_id = p.product_id**: This joins the sales table with the products table using the product_id to get additional information about the products sold.\n",
      "\n",
      "5. **WHERE s.sale_date >= DATEADD(qq, -1, GETDATE()) AND s.sale_date < DATEADD(qq, 0, GETDATE())**: This filters the sales data to include only those that occurred in the last quarter. The `DATEADD(qq, -1, GETDATE())` gets the start of the last quarter, and `DATEADD(qq, 0, GETDATE())` gets the start of the current quarter.\n",
      "\n",
      "6. **AND p.name LIKE '%Canada%'**: This further filters the results to include only those products whose names contain the word \"Canada\", indicating that the goods were sold in Canada.\n",
      "\n",
      "By combining these elements, the SQL query effectively calculates the total number of goods sold in Canada during the last quarter.\n",
      "\n",
      "```sql\n",
      "SELECT SUM(s.quantity) AS total_goods_sold\n",
      "FROM sales s\n",
      "JOIN products p ON s.product_id = p.product_id\n",
      "WHERE s.sale_date >= DATEADD(qq, -1, GETDATE())\n",
      "AND s.sale_date < DATEADD(qq, 0, GETDATE())\n",
      "AND p.name LIKE '%Canada%';\n",
      "\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(generated_sql)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
